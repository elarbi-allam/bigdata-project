from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, avg, max, min, count
from datetime import datetime

spark = SparkSession.builder \
    .appName("IoT Analysis") \
    .master("local[2]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

print("=" * 70)
print("ğŸ“Š ANALYSE DES DONNÃ‰ES IoT")
print("=" * 70)

try:
    # Lire les donnÃ©es
    df = spark.read.parquet("/tmp/iot-data/raw/*.parquet")
    
    total = df.count()
    villes = df.select('city').distinct().count()
    capteurs = df.select('sensor_id').distinct().count()
    
    print(f"\nâœ… {total:,} lectures analysÃ©es")
    print(f"ğŸ“ {villes} villes surveillÃ©es")
    print(f"ğŸ”§ {capteurs} capteurs actifs\n")
    
    # Statistiques globales
    print("=" * 70)
    print("ğŸŒ¡ï¸  STATISTIQUES GLOBALES")
    print("=" * 70)
    
    stats = df.select(
        avg("temperature").alias("temp_moy"),
        max("temperature").alias("temp_max"),
        min("temperature").alias("temp_min"),
        avg("humidity").alias("hum_moy")
    ).collect()[0]
    
    print(f"TempÃ©rature moyenne : {stats['temp_moy']:.2f}Â°C")
    print(f"TempÃ©rature max     : {stats['temp_max']:.2f}Â°C")
    print(f"TempÃ©rature min     : {stats['temp_min']:.2f}Â°C")
    print(f"HumiditÃ© moyenne    : {stats['hum_moy']:.2f}%")
    
    # Par ville
    print("\n" + "=" * 70)
    print("ğŸ“ ANALYSE PAR VILLE")
    print("=" * 70)
    
    df.groupBy("city") \
        .agg(
            avg("temperature").alias("temp_moy"),
            max("temperature").alias("temp_max"),
            min("temperature").alias("temp_min"),
            count("*").alias("lectures")
        ) \
        .orderBy(desc("temp_moy")) \
        .show(truncate=False)
    
    # Alertes
    print("=" * 70)
    print("âš ï¸  ALERTES")
    print("=" * 70)
    
    hot = df.filter(col("temperature") > 40).count()
    cold = df.filter(col("temperature") < 20).count()
    
    print(f"ğŸ”¥ TempÃ©ratures > 40Â°C : {hot}")
    print(f"â„ï¸  TempÃ©ratures < 20Â°C : {cold}")
    
    if hot > 0:
        print("\nğŸ”¥ Top 5 tempÃ©ratures Ã©levÃ©es :")
        df.filter(col("temperature") > 40) \
            .select("city", "temperature", "timestamp") \
            .orderBy(desc("temperature")) \
            .show(5, truncate=False)
    
    # GÃ©nÃ©rer rapport
    rapport = f"""# RAPPORT IoT - {datetime.now().strftime('%d/%m/%Y %H:%M')}

## RÃ©sumÃ©
- **Lectures** : {total:,}
- **Villes** : {villes}
- **Capteurs** : {capteurs}

## Statistiques
- Temp. moyenne : {stats['temp_moy']:.2f}Â°C
- Temp. max : {stats['temp_max']:.2f}Â°C
- Temp. min : {stats['temp_min']:.2f}Â°C
- HumiditÃ© moy. : {stats['hum_moy']:.2f}%

## Alertes
- ğŸ”¥ > 40Â°C : {hot}
- â„ï¸ < 20Â°C : {cold}

**Projet Big Data - ENSA 2024/2025**
"""
    
    with open("/tmp/rapport.md", "w") as f:
        f.write(rapport)
    
    print("\n" + "=" * 70)
    print("âœ… Rapport sauvegardÃ© : /tmp/rapport.md")
    print("ğŸ“‹ Copier : docker cp spark-master:/tmp/rapport.md ./")
    print("=" * 70)
    
except Exception as e:
    print(f"âŒ Erreur : {e}")

spark.stop()